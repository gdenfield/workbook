{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1. \n",
    "###Using Neural Networks to Recognize Handwritten Digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter we discuss some basics of neural networks, including two types of neuron analogs, the perceptron and the sigmoid neuron, a method for training networks, stochastic gradient descent, and build a simple network to recognize handwritten digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perceptrons**\n",
    "\n",
    "Perceptrons function quite simply. They take binary inputs and combine them to provide a binary output, through a simple linear operation. For example, given inputs $x_1$, $x_2$, $x_3$, the perceptron will perform a weighted sum of these inputs $\\sum_jw_jx_j$ and respond with a 0 if the result is below a threshold value or a 1 if it is above a threshold value. To improve our notation, however, we use the following for out output equation:\n",
    "\n",
    "$$\n",
    "\\text{output}=\n",
    "\\begin{cases}\n",
    "0, \\mbox{ if } \\mathbf{w}^T\\mathbf{x} + b \\leq 0 \\\\\n",
    "1, \\mbox{ if } \\mathbf{w}^T\\mathbf{x} + b > 0,\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where $b$ is the *bias,* which is the negative of our threshold.\n",
    "\n",
    "One useful feature of the perceptron is that it can compute any logical function, for example AND, OR, or NAND by adjusting the weights and threshold appropriately. A NAND gate can be constructed, for example, by taking the two weights to be -2 and the bias to be +3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def nand(x1, x2):\n",
    "    output = 1\n",
    "    if (-2*x1 + -2*x2 + 3) < 0:\n",
    "        output = 0\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nand(1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One major shortcoming of the perceptron is that it is not clear how to get a network of perceptrons to learn in a stable manner. That is, ideally if the network's output is incorrect, we could slightly modify some subset of the weights to produce a small change in the output. However, weight modification in perceptron networks is a risky business, as the output is likely to flip from 0 to 1 with slight weight modifications and to have widespread effects on the output class for a variety of inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sigmoid Neurons**\n",
    "\n",
    "Sigmoid neurons, also called logistic neurons, overcome this problem by taking any value in the range of 0 to 1. A sigmoid neuron's output is given by\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1+e^{-z}} = \\frac{1}{1+e^{-(\\mathbf{w}^T\\mathbf{x}+b)}}\n",
    "$$\n",
    "\n",
    "Using the sigmoid *activation function* now allows for small changes in weights and biases to have small effects on the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Delta y \\approx \\sum_j\\frac{\\partial y}{\\partial w_j}\\Delta w_j + \\frac{\\partial y}{\\partial b}\\Delta b\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
