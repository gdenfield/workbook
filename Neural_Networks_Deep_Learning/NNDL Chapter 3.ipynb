{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter we focus on improvements to the methods outlined in the previous two chapters. For example, we examine an \"improved\" cost function, and different regularization methods that emphasize generalization beyond the specific training data set.\n",
    "\n",
    "### Cross-Entropy Cost Function\n",
    "\n",
    "In many instances of human learning, the larger the error one makes, the more quickly corrected it is, or at least, the more readily recognized it is wrong and that something needs to be done about it. Interestingly, with our previously described cost function used in the most basic backpropagation algorithms, the same behavior is not present. Whereas humans often learn fastest when they are badly wronge, neural networks will often learn slowly in these circumstances."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
