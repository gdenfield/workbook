{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains notes and examples pertaining to Generalized Linear Models. Most of the discussion will be drawn from the book: McCullagh and Nedler (1989) Generalized Linear Models.\n",
    "\n",
    "### Chapter 1: Introduction\n",
    "\n",
    "**Model Fitting**\n",
    "\n",
    "Say we have a model with two parameters, $(a,b)$. For model fitting, we need to find the pair of parameters amongst all possible pairs that most closely generates the $y$ values we observed as responses to the predictor. We thus need a measure of 'closeness,' where a number of options are available. Most typically used for least squares is the $L_2$-norm, which is the typical sum of squares. Other options follow as well.\n",
    "\n",
    "\\begin{align}\n",
    "L_1: S_1(y,\\hat{y}) = \\sum|y_i - \\hat{y}_i| \\\\\n",
    "L_2: S_2(y,\\hat{y}) = \\sum(y_i - \\hat{y}_i)^2 \\\\\n",
    "L_{\\inf}: S_{\\inf}(y,\\hat{y}) = \\max_i|y_i - \\hat{y}_i|\n",
    "\\end{align}\n",
    "\n",
    "Norms using the straightforward summation of individual deviations imply that each observation can be treated separately and weighted equally to the others. This treatment is related to the assumption of independence of observations and uncorrelatedness of error. \n",
    "\n",
    "The classical least squares criterion arises from regarding the $x$-values as fixed and the $y$-values have the Normal distribution, such that the probability or frequency of $y$ given $x$ is \n",
    "\n",
    "$$ \n",
    "\\propto \\exp\\Big(\\frac{-(y-\\mu)^2}{2\\sigma^2}\\Big),\n",
    "$$\n",
    "\n",
    "where $x$ is linearly related to $\\mu$ through $a$ and $b$. We can look at the above function in two ways. First, we see it as a function of fixed $\\mu$, in which case it gives the probability of $y$. However, we can also view it for a given value of $y$ as a function of $\\mu$, giving the plausibility of different values of $\\mu$ for a value of $y$. This is the *likelihood function* view."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 2: An Outline of GLMs\n",
    "\n",
    "*Model Selection*\n",
    "\n",
    "1. Assume independence of observations\n",
    "2. Assume existence of only one error term\n",
    "\n",
    "*Estimation*\n",
    "\n",
    "Once a model is selected we must estimate the parameters and the precisions thereof. In general, our approach will be to design some goodness-of-fit measure and find the parameter values that minimized this measure. Primarily, we will use maximum likelihood estimation and work with the log-likelihood function. If our density function is given by $f(y;\\theta)$ then our likelihood function is $f(\\theta;y)$, where if we have multiple, independent observations $y_1,...y_n$, the log likelihood for the set is\n",
    "\n",
    "$$\n",
    "l(\\boldsymbol{\\theta},\\mathbf{y}) = \\sum_i\\log f_i(y_i;\\theta_i)\n",
    "$$\n",
    "\n",
    "Sometimes the *scaled deviance* may be preferable to the log-likelihood:\n",
    "\n",
    "$$\n",
    "D^*(\\mathbf{y};\\boldsymbol{\\theta}) = 2l(\\mathbf{y};\\mathbf{y}) - 2l(\\boldsymbol{\\theta};\\mathbf{y})\n",
    "$$\n",
    "\n",
    "The $l(y;y)$ term is the maximum likelihood achievable for an exact fit of the observed data. For exponential family models, this term does not depend on the mean parameter, $\\mathbf{\\mu}$ so maximizing the likelihood minimizes the scaled deviance.\n",
    "\n",
    "**The Components of a GLM**\n",
    "\n",
    "For classical linear models we have a vector of $n$ observations, denoted $\\mathbf{Y}$, whose components are independently distributed with mean $\\boldsymbol{\\mu} = \\sum_{j=1}^p \\mathbf{x}_j\\beta_j$, and we assume the random component is independent and with constant variance. We may make a stronger assumption, such as that the error is Normally distributed.\n",
    "\n",
    "For a GLM we begin by expanding on the above assumptions with the following three components:\n",
    "\n",
    "1. The *random component*: The components of $\\mathbf{Y}$ have independent Normal distributions with mean $E[\\mathbf{Y}] = \\boldsymbol{\\mu}$ and constant variance $\\sigma^2$.\n",
    "2. The *systematic component*: The covariates $\\mathbf{x_1, x_2, ..., x_p}$ produe a linear predictor $\\eta$:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\eta} = \\sum_{i=1}^p \\mathbf{x}_i\\beta_i\n",
    "$$\n",
    "\n",
    "3. The *link function*, $g$, between the random and systematic components, which for the classical linear model is the identity: $g(\\mu_i) = \\eta$\n",
    "\n",
    "For GLMs, 1 may involve any exponential family distribution and 3 may involve any monotonic, differentiable function.\n",
    "\n",
    "*Likelihood functions for GLMs*\n",
    "\n",
    "We assume the components of $\\mathbf{Y}$ take an exponential family distribution in the form of: \n",
    "\n",
    "$$\n",
    "f_Y(y;\\theta,\\phi) = \\exp\\{\\big(y\\theta - b(\\theta)\\big)\\div a(\\phi) + c(y,\\theta)\\}\n",
    "$$\n",
    "\n",
    "The log likelihood function for the above distribution can be specified as $l(\\theta,\\phi;y) = \\log f_Y(y;\\theta,\\phi)$, giving:\n",
    "\n",
    "$$\n",
    "l(\\theta,\\phi; y) = \\frac{y\\theta - b(\\theta)}{a(\\phi)} + c(y,\\theta)\n",
    "$$\n",
    "\n",
    "\n",
    "The mean, $E[Y]$, and variance, $E[Y^2] + E[Y]^2$, of $Y$ are derived using \n",
    "\n",
    "\\begin{align}\n",
    "E\\Bigg[\\frac{\\partial l}{\\partial \\theta} \\Bigg] &= 0 \\\\\n",
    "E\\Bigg[\\frac{\\partial^2 l}{\\partial \\theta^2} \\Bigg] + E\\Bigg[\\frac{\\partial l}{\\partial \\theta} \\Bigg]^2 &= 0 \\\\\n",
    "\\frac{\\partial l}{\\partial \\theta} &= \\{y - b'(\\theta)\\}/a(\\phi) \\\\\n",
    "\\frac{\\partial^2 l}{\\partial \\theta^2} &= b''(\\theta) \\ a(\\phi) \\\\\n",
    "\\end{align}\n",
    "\n",
    "as follows:\n",
    "\n",
    "\\begin{align}\n",
    "E\\Big[\\frac{\\partial l}{\\partial \\theta}\\Big] &= \\{\\mu - b'(\\theta)\\}/a(\\phi) = 0 \\\\\n",
    "\\mu &= b'(\\theta) \\\\\n",
    "E\\Bigg[\\frac{\\partial^2 l}{\\partial \\theta^2} \\Bigg] + E\\Bigg[\\frac{\\partial l}{\\partial \\theta} \\Bigg]^2 &= b''(\\theta) \\ a(\\phi) + \\Big[\\{y - \\mu\\}/a(\\phi)\\Big]^2 =  b''(\\theta) \\ a(\\phi) + Var(Y)/a(\\phi)^2= 0 \\\\\n",
    "Var(Y) &= a(\\phi)b''(\\theta)\n",
    "\\end{align}\n",
    "\n",
    "We call $\\theta$ the canonical parameter. The variance of $Y$ is a product of two functions, one, which is $b''(\\theta)$ is the *variance funciton* and depends on the canonical parameter (and hence on the mean), while the other is independent of $\\theta$.\n",
    "\n",
    "*The Link Function*\n",
    "\n",
    "Models with multiplicative effects are expressed naturally using the *log-link*, for example, when working with Poisson distributed variables.\n",
    "\n",
    "For the binomial distribution, we require that $0 \\lt \\mu \\lt 1$, so we need a link function that maps this interval to the real line. There are several options (to be discussed in greater detail in coming chapters), but they include\n",
    "\n",
    "1. Logit: $\\eta = \\log\\frac{\\mu}{1-\\mu}$\n",
    "2. Probit: $\\eta = \\Phi^{-1}(\\mu)$\n",
    "3. Complementary Log-Log: $\\eta = \\log\\{-\\log(1-\\mu)\\}$\n",
    "\n",
    "Another important class of link functions is the *power family*, which can be written as\n",
    "\n",
    "$$\n",
    "\\eta = \\frac{\\mu^{\\lambda}-1}{\\lambda}\n",
    "$$\n",
    "\n",
    "or \n",
    "\n",
    "$$\n",
    "\\eta =\n",
    " \\begin{cases} \n",
    "     \\hfill \\mu^{\\lambda}    \\hfill & \\lambda \\neq 0 \\\\\n",
    "     \\hfill \\log \\mu         \\hfill & \\lambda = 0 \\\\\n",
    " \\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "*Sufficient Statistics and Canonical Links*\n",
    "\n",
    "A *canonical link* function is one for which there exists a sufficient statistic equal in dimension to $\\boldsymbol{\\beta}$ in the linear predictor $\\boldsymbol{\\eta} = \\sum_j\\mathbf{x}_j\\beta_j$. They occur when $\\eta = \\theta$. Some canonical links include:\n",
    "\n",
    "1. Normal $\\eta = \\mu$\n",
    "2. Poisson $\\eta = \\log\\mu$\n",
    "3. Binomial $\\eta = \\log\\frac{\\pi}{1-\\pi}$\n",
    "4. Gamma $\\eta = \\mu^{-1}$\n",
    "5. Inverse Gaussian $\\eta = \\mu^{-2}$\n",
    "\n",
    "For the canonical links, the sufficient statistic is $\\mathbf{X}'\\mathbf{Y}$.\n",
    "\n",
    "*Measuring the Goodness of Fit*\n",
    "\n",
    "Primarily we will use the log of a ratio of likelihoods, to be called the deviance. A dataset of $n$ observations can be modeled with up to $n$ parameters. The *null model* has just one parameter, which is equal to $\\mu$, the mean of the $y$'s, and assigns all variability in the $y$'s to the random component. On the other extreme is the model with $n$ parameters, the *full model*, which captures the $y$'s perfectly, leaving no variability to the random component. This model is too rigid and does not summarize (but repeats) the data. However, it does serve as a baseline for comparison of intermediate models.\n",
    "\n",
    "The discrepancy of a fit is proportional to twice the difference between the maximum log likelihood achievable and that achievable with the model under consideration. The maximum log likelihood achievable is written as $l(\\mathbf{y},\\phi; \\mathbf{y})$. Now, we write the estimates of our canonical parameter under each model as $\\boldsymbol{\\hat{\\theta}} = \\boldsymbol{\\theta(\\hat{\\mu})}$ and $\\boldsymbol{\\tilde{\\theta}} = \\boldsymbol{\\theta(y)}$ and, for now, assume that $a_i(\\phi) = \\frac{\\phi}{w_i}$, then we can write the discrepancy as\n",
    "\n",
    "$$\n",
    "D(\\mathbf{y},\\boldsymbol{\\hat{\\mu}})/\\phi = \\sum 2w_i\\{y_i(\\tilde{\\theta} - \\hat{\\theta}) - b(\\tilde{\\theta}) + b(\\hat{\\theta})\\},\n",
    "$$,\n",
    "\n",
    "where the quantitiy $D(\\mathbf{y},\\boldsymbol{\\hat{\\mu}})/\\phi$ is called the *Deviance*, and is proportional to the *scaled deviance* $D^*(\\mathbf{y},\\boldsymbol{\\hat{\\mu}}) = D(\\mathbf{y},\\boldsymbol{\\hat{\\mu}})/\\phi$.\n",
    "\n",
    "Forms for the Deviance for a number of exponential family distributions are given on page 34 (Preview p49) of the text. Some are listed below\n",
    "\n",
    "1. Normal: $\\sum(y-\\hat{\\mu})^2$\n",
    "2. Poisson: $2\\sum\\{y\\log\\frac{y}{\\hat{\\mu}} - (y-\\hat{\\mu})\\}$\n",
    "\n",
    "The generalized Pearson $\\chi^2$ statistic, which takes the form \n",
    "\n",
    "$$\n",
    "\\chi^2 = \\frac{(y-\\hat{\\mu})^2}{V(\\hat{\\mu})},\n",
    "$$\n",
    "\n",
    "where the denominator is the variance function, and is also used as a measure of discrepancy. Rather than performing analyses of variance, analysis of deviance is performed. The general procedure is to work through nested models, moving from the most reduced to the fullest and taking account of the reduction in sum of squares with each new term. One issue is that the model terms are non-orthogonal and so different sequences of term testing may impact that sum of squares differently. This approach should be used as a screening procedure for important terms in the model and not for purposes of exact inference.\n",
    "\n",
    "*Residuals*\n",
    "\n",
    "We need to generalize the concept of residuals to function for non-Normal models. In the Normal case, residuals can be written as $(y-\\hat{\\mu})$ and the datum $y = \\hat{\\mu} + (y-\\hat{\\mu})$, the fitted value plus the residual. The residual then functions as a measure of goodness-of-fit of the model. Several residuals can be used for GLMs.\n",
    "\n",
    "*Pearson Residuals* defined as $r_p = \\frac{y - \\hat{\\mu}}{\\sqrt(V(\\hat{\\mu})}$ is the raw residual scaled by the estimated standard deviation of $Y$.\n",
    "\n",
    "The Pearson residual distribution is often skewed, and therefore may not possess asymptotic Normal properties. An alternative is to use a transformation of $Y, A(Y)$, that makes the resulting distribution as Normal as possible. This transformation differs depending on the distribution used. Refer to *Anscombe Residuals* for further information.\n",
    "\n",
    "*Deviance Residuals* are often used when the deviance is used as a measure of goodness-of-fit. Each unit contributes $d_i$ to the discrepancy measure, and so $\\sum d_i = D$. We can define\n",
    "\n",
    "$$\n",
    "r_D = \\frac{sign(y - \\hat{\\mu})}{\\sqrt d_i},\n",
    "$$\n",
    "\n",
    "and we have a quantity that increases with $y-\\hat{\\mu}$ and where $\\sum r_D^2 = D$.\n",
    "\n",
    "*An Algorithm for Fitting GLMs*\n",
    "\n",
    "Iterative weighted least squares can be used to find maximum-likelihood estimates of the paramters $\\boldsymbol{\\beta}$ in the linear predictor $\\eta$. The dependent variable in this regression is $z$, a linearized version of the link function applied to $y$, and the weights are functions of the fitted values $\\hat{\\mu}$. We use the first order approximation of $\\eta = g(\\mu), g(\\mu) \\approx g(\\mu) + (y-\\mu)g'(\\mu) = \\eta + (y-\\mu)\\frac{\\partial \\eta}{\\partial \\mu}$.\n",
    "\n",
    "The procedure starts with an initial estimate of the linear predictor, $\\hat{\\eta}_0$, and fitted value $\\hat{\\mu}_0$ derived from the link function. We calculate\n",
    "\n",
    "\\begin{align}\n",
    "z_0 &= \\eta_0 + (y-\\mu)\\Big(\\frac{\\partial \\eta}{\\partial \\mu}\\Big)_0 \\\\\n",
    "W_0^{-1} &= \\Big(\\frac{\\partial \\eta}{\\partial \\mu}\\Big)^2V_0\n",
    "\\end{align}\n",
    "\n",
    "We then regress $z_0$ on the covariates $x_i$ with weight $W_0$ to get new estimates $\\boldsymbol{\\hat{\\beta}}_1$. This procedure is repeated until differences between successive estimates are sufficiently small. We use the data to form the estiamte $\\hat{\\mu}_0$. \n",
    "\n",
    "Maximum likelihood equations for $\\beta_j$ take the form:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial l}{\\partial \\beta_j} &= \\frac{\\partial l}{\\partial \\theta}\\frac{\\theta}{\\partial \\mu}\\frac{\\partial \\mu}{\\partial \\eta}\\frac{\\partial \\eta}{\\partial \\beta_j} \\\\\n",
    "&= \\frac{y-\\mu}{a(\\phi)}\\frac{1}{V}\\frac{\\partial \\mu}{\\partial \\eta}x_j \\\\\n",
    "&= \\frac{W}{a(\\phi)}(y-\\mu)\\frac{\\partial \\eta}{\\partial \\mu}x_j,\n",
    "\\end{align}\n",
    "\n",
    "using our definition of $W, b'(\\theta) = \\mu, b''(\\theta) = V$. With constant dispersion this gives:\n",
    "\n",
    "$$\n",
    "\\sum W(y-\\mu)\\frac{\\partial \\eta}{\\partial \\mu}x_j = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###Chapter 3 Models for Continuous Data with Constant Variance\n",
    "\n",
    "In general, the most important assumptions for linear models are the first and second-moment assumptions that $E[\\mathbf{Y}] = \\boldsymbol{\\mu}$ and $Cov[\\mathbf{Y}] = \\sigma^2\\mathbf{I}$. The constancy of error variance assumption is particularly important and should be checked. \n",
    "\n",
    "In all generalized linear models, we have as our linear predictor\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\eta} = \\sum_j\\mathbf{x}_j\\beta_j\n",
    "$$\n",
    "\n",
    "Models with only *continuous covariates*, $\\mathbf{x}_j$, are called *regression models*, whereas those with only qualitative factors are called *analysis-of-variance* models. \n",
    "\n",
    "Linearity in the present context refers to the linearity of $\\eta$ in the parameters. Thus, we can use functions of $\\eta$, such as $\\exp(\\eta)$ without violating this linearity.\n",
    "\n",
    "*Aliasing*\n",
    "\n",
    "Each term in a model defines a set of covariates that span a subspace. In some cases the dimensionality of this subspace is less than maximal (ie if not all vectors are linearly independent). Further, terms in a GLM may have partially to completely overlapping subspaces, and this can produce *aliasing*. In this case, certain combinations of covariates are identical to certain other combinations.\n",
    "\n",
    "**Estimation**\n",
    "\n",
    "Maximum likelihood is the principal method of estimation for GLMs. Normal error models give the likelihood equation:\n",
    "\n",
    "$$\n",
    "-2l = n\\log(2\\pi\\sigma^2) + \\frac{\\sum_i(y_i - \\mu_i)^2}{\\sigma^2}\n",
    "$$\n",
    "\n",
    "Now we assume the model is linear,\n",
    "\n",
    "$$\n",
    "\\eta_i = \\mu_i = \\sum_j^p x_{ij}\\beta_j,\n",
    "$$\n",
    "\n",
    "and differentiate with respect to $\\beta_j$ to get\n",
    "\n",
    "\\begin{align}\n",
    "\\sum_ix_{ij}(y_i-\\hat{\\mu}_i) &= 0 \\\\\n",
    "\\mu_i = \\eta_i &= \\sum_j^p x_{ij}\\beta_j,\n",
    "\\end{align}\n",
    "\n",
    "One way to think of the above result is that the linear combination of observations $\\sum_ix_{ij}y_i$ is set equal to the linear combination of fitted values $\\sum_ix_{ij}\\hat{\\mu}_i$. Equivalently, the vector of residuals $\\mathbf{e}$ is orthogonal to the columns of the data matrix $\\mathbf{X}$. $\\mathbf{X}'\\mathbf{e} = \\mathbf{X}'(\\mathbf{y}-\\hat{\\boldsymbol{\\mu}}) = \\mathbf{0}$.\n",
    "\n",
    "*Geometrical Interpretation of Estimation of Model Parameters*\n",
    "\n",
    "The data vector $\\mathbf{y}$ is a point in the *n*-dimensional observation space, as is the vector $\\boldsymbol{\\mu} = \\mathbf{X}\\boldsymbol{\\beta}$. As $\\boldsymbol{\\beta}$ varies over its range of possible values, $\\boldsymbol{\\mu}$ traces out a linear subspace called the *solution locus*. Maximizing the likelihood, or minimizing the least squares in this case, is equivalent to finding the point $\\boldsymbol{\\mu}$ that is closest, in a Euclidian distance sense, to $\\mathbf{y}$. We see that the fitted vector $\\mathbf{x}\\hat{\\beta}$ is the orthogonal projection of $\\mathbf{y}$ onto $\\mathbf{x}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
